{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c06b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense , Dropout,Flatten,Rescaling\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit.primitives import Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6ed08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MildDemented Folder has 1000 Images\n",
      "ModerateDemented Folder has 1000 Images\n",
      "NonDemented Folder has 1000 Images\n",
      "VeryMildDemented Folder has 1000 Images\n",
      "Images folder has 4000 Images\n"
     ]
    }
   ],
   "source": [
    "dirs = os.listdir(\"Sampled_images/\")\n",
    "count = 0\n",
    "for dir in dirs:\n",
    "    files = list(os.listdir(\"Sampled_images/\" + dir + \"/\")) \n",
    "    print( dir + \" Folder has \" + str(len(files))+ \" Images\")\n",
    "    count = count + len(files)\n",
    "    \n",
    "print(\"Images folder has \" + str(count) + \" Images\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0e5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"Sampled_images/\"\n",
    "img_size = 180\n",
    "batch = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c4f3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 files belonging to 4 classes.\n",
      "Using 3200 files for training.\n",
      "Found 4000 files belonging to 4 classes.\n",
      "Using 800 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    base_dir,\n",
    "    seed=123,  \n",
    "    validation_split=0.2,  \n",
    "    subset=\"training\",  \n",
    "    batch_size=batch,  \n",
    "    image_size=(img_size, img_size)\n",
    "    \n",
    ") \n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    base_dir,\n",
    "    seed=123,  \n",
    "    validation_split=0.2,  \n",
    "    subset=\"validation\",  \n",
    "    batch_size=batch,  \n",
    "    image_size=(img_size, img_size)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f48f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from qiskit.circuit import ParameterVector, QuantumCircuit\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit.primitives import Estimator\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "from qiskit.primitives import Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d8d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from qiskit.circuit import ParameterVector, QuantumCircuit\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit.primitives import Estimator\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "# === Quantum Circuit Function (6 qubits) ===\n",
    "def create_multiclass_pqc(num_qubits=6):\n",
    "    x = ParameterVector(\"x\", num_qubits)\n",
    "    theta = ParameterVector(\"θ\", num_qubits)\n",
    "\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "\n",
    "    # Angle encoding\n",
    "    for i in range(num_qubits):\n",
    "        qc.h(i)\n",
    "        qc.ry(x[i], i)\n",
    "\n",
    "    # Entanglement\n",
    "    for i in range(num_qubits - 1):\n",
    "        qc.cz(i, i + 1)\n",
    "\n",
    "    # Trainable weights\n",
    "    for i in range(num_qubits):\n",
    "        qc.ry(theta[i], i)\n",
    "\n",
    "    # Measurement observables (Z per qubit)\n",
    "    observables = [SparsePauliOp('I' * i + 'Z' + 'I' * (num_qubits - i - 1)) for i in range(num_qubits)]\n",
    "\n",
    "    return qc, list(x), list(theta), observables\n",
    "\n",
    "# === Multi-Class Classical-Quantum CNN (6 Qubits) ===\n",
    "class MultiClassCQCNN(nn.Module):\n",
    "    def __init__(self, num_qubits=6, num_classes=4):\n",
    "        super(MultiClassCQCNN, self).__init__()\n",
    "        self.num_qubits = num_qubits\n",
    "\n",
    "        # Classical Feature Extractor for Grayscale (1-channel)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # 180x180 → 180x180\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # → 90x90\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # → 90x90\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # → 45x45\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # → 45x45\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(5),  # → 9x9\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 9 * 9, num_qubits)  # Reduce to quantum input size (6)\n",
    "\n",
    "        # Quantum Circuit & QNN\n",
    "        qc, input_params, weight_params, observables = create_multiclass_pqc(num_qubits)\n",
    "        estimator = Estimator()\n",
    "        qnn = EstimatorQNN(\n",
    "            circuit=qc,\n",
    "            input_params=input_params,\n",
    "            weight_params=weight_params,\n",
    "            observables=observables,\n",
    "            estimator=estimator,\n",
    "            input_gradients=True\n",
    "        )\n",
    "\n",
    "        # Torch-Quantum Bridge\n",
    "        self.q_layer = TorchConnector(qnn)\n",
    "\n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Linear(num_qubits, num_classes)  # 6 → 4 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)         # Classical CNN\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)                 # To quantum input size (6 features)\n",
    "        x = torch.tanh(x)               # Normalize input to range [-1, 1]\n",
    "        x = self.q_layer(x)             # Quantum Layer (6 outputs)\n",
    "        x = self.classifier(x)          # Output logits (4 classes for Alzheimer's stages)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "619b03de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassCQCNN(nn.Module):\n",
    "    def __init__(self, num_qubits=4, num_classes=4):\n",
    "        super(MultiClassCQCNN, self).__init__()\n",
    "        self.num_qubits = num_qubits\n",
    "\n",
    "        # Classical Feature Extractor with Batch Normalization\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(5),\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 9 * 9, num_qubits)\n",
    "\n",
    "        # Quantum Circuit & QNN\n",
    "        qc, input_params, weight_params, observables = create_multiclass_pqc(num_qubits)\n",
    "        \n",
    "        # Fixed: Use SamplerQNN for better stability\n",
    "        sampler = Sampler()\n",
    "        qnn = SamplerQNN(\n",
    "            circuit=qc,\n",
    "            input_params=input_params,\n",
    "            weight_params=weight_params,\n",
    "            sampler=sampler,\n",
    "            input_gradients=True\n",
    "        )\n",
    "\n",
    "        # Torch-Quantum Bridge\n",
    "        self.q_layer = TorchConnector(qnn)\n",
    "        \n",
    "        # Fixed: Initialize quantum weights\n",
    "        self._init_quantum_weights()\n",
    "\n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Linear(2**num_qubits, num_classes)  # SamplerQNN outputs 2^n values\n",
    "\n",
    "    def _init_quantum_weights(self):\n",
    "        \"\"\"Initialize quantum layer weights\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Small random initialization for quantum parameters\n",
    "            if hasattr(self.q_layer, 'weight') and self.q_layer.weight is not None:\n",
    "                nn.init.uniform_(self.q_layer.weight, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        # Fixed: Apply tanh first, then scale to [-π, π]\n",
    "        x = torch.tanh(x) * torch.pi\n",
    "        x = self.q_layer(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e316745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassCQCNN(nn.Module):\n",
    "    def __init__(self, num_qubits=4, num_classes=4):\n",
    "        super(MultiClassCQCNN, self).__init__()\n",
    "        self.num_qubits = num_qubits\n",
    "\n",
    "        # Classical Feature Extractor for Grayscale (1-channel)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # 180x180 → 180x180\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # → 90x90\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # → 90x90\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # → 45x45\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # → 45x45\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(5),  # → 9x9\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 9 * 9, num_qubits)  # Reduce to quantum input size\n",
    "\n",
    "        # Quantum Circuit & QNN\n",
    "        qc, input_params, weight_params, observables = create_multiclass_pqc(num_qubits)\n",
    "        estimator = Estimator()\n",
    "        qnn = EstimatorQNN(\n",
    "            circuit=qc,\n",
    "            input_params=input_params,\n",
    "            weight_params=weight_params,\n",
    "            observables=observables,\n",
    "            estimator=estimator,\n",
    "            input_gradients=True\n",
    "        )\n",
    "\n",
    "        # Torch-Quantum Bridge\n",
    "        self.q_layer = TorchConnector(qnn)\n",
    "\n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Linear(num_qubits, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)         # Classical CNN\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)                 # To quantum input size\n",
    "        x = torch.tanh(x)               # Normalize input to range [-1, 1]\n",
    "        x = self.q_layer(x)             # Quantum Layer\n",
    "        x = self.classifier(x)          # Output logits\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99c3d2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClassCQCNN(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=5184, out_features=6, bias=True)\n",
      "  (q_layer): TorchConnector()\n",
      "  (classifier): Linear(in_features=6, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from qiskit.quantum_info.operators import SparsePauliOp\n",
    "\n",
    "model = MultiClassCQCNN(num_qubits=6)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5154db9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total trainable parameters: 54440\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "862a5bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiClassCQCNN(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=5184, out_features=6, bias=True)\n",
       "  (q_layer): TorchConnector()\n",
       "  (classifier): Linear(in_features=6, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60f705bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf340393",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbf9823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# ✅ Define transform with grayscale conversion\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # ⬅️ Ensure grayscale\n",
    "    transforms.Resize((180, 180)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# ✅ Load dataset\n",
    "base_dir = \"Sampled_images\"\n",
    "full_dataset = datasets.ImageFolder(root=base_dir, transform=transform)\n",
    "\n",
    "# ✅ Train/Validation split\n",
    "val_split = 0.2\n",
    "val_size = int(len(full_dataset) * val_split)\n",
    "train_size = len(full_dataset) - val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# ✅ Create DataLoaders\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e20da81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 1.3185 | Train Acc: 0.3416 | Val Loss: 1.1423 | Val Acc: 0.5238\n",
      "Epoch 2/100 | Train Loss: 1.0158 | Train Acc: 0.5784 | Val Loss: 0.9066 | Val Acc: 0.6462\n",
      "Epoch 3/100 | Train Loss: 0.8363 | Train Acc: 0.6444 | Val Loss: 0.9016 | Val Acc: 0.6012\n",
      "Epoch 4/100 | Train Loss: 0.7482 | Train Acc: 0.6781 | Val Loss: 0.7436 | Val Acc: 0.6675\n",
      "Epoch 5/100 | Train Loss: 0.6600 | Train Acc: 0.7191 | Val Loss: 0.7281 | Val Acc: 0.6887\n",
      "Epoch 6/100 | Train Loss: 0.6011 | Train Acc: 0.7444 | Val Loss: 0.6546 | Val Acc: 0.7225\n",
      "Epoch 7/100 | Train Loss: 0.5310 | Train Acc: 0.7822 | Val Loss: 0.6710 | Val Acc: 0.7163\n",
      "Epoch 8/100 | Train Loss: 0.5051 | Train Acc: 0.8022 | Val Loss: 0.6559 | Val Acc: 0.7113\n",
      "Epoch 9/100 | Train Loss: 0.4267 | Train Acc: 0.8391 | Val Loss: 0.6157 | Val Acc: 0.7388\n",
      "Epoch 10/100 | Train Loss: 0.3707 | Train Acc: 0.8644 | Val Loss: 0.6401 | Val Acc: 0.7400\n",
      "Epoch 11/100 | Train Loss: 0.3393 | Train Acc: 0.8800 | Val Loss: 0.5932 | Val Acc: 0.7688\n",
      "Epoch 12/100 | Train Loss: 0.2984 | Train Acc: 0.8959 | Val Loss: 0.6072 | Val Acc: 0.7575\n",
      "Epoch 13/100 | Train Loss: 0.2629 | Train Acc: 0.9087 | Val Loss: 0.7161 | Val Acc: 0.7250\n",
      "Epoch 14/100 | Train Loss: 0.2253 | Train Acc: 0.9259 | Val Loss: 0.6416 | Val Acc: 0.7538\n",
      "Epoch 15/100 | Train Loss: 0.1885 | Train Acc: 0.9434 | Val Loss: 0.6528 | Val Acc: 0.7738\n",
      "Epoch 16/100 | Train Loss: 0.1890 | Train Acc: 0.9400 | Val Loss: 0.7524 | Val Acc: 0.7462\n",
      "Early stopping at epoch 16\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "patience = 5\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_wts = None\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Training loop\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # logits output\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss = val_loss / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_wts = deepcopy(model.state_dict())\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best model weights after early stopping\n",
    "if best_model_wts:\n",
    "    model.load_state_dict(best_model_wts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
